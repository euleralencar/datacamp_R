---
title: "Análise Fatorial"
author: "Euler Alencar"
date: "03/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
# Load the psych package-----
require(tidyverse)
require(psych)

# Download data--------------
link = 'https://assets.datacamp.com/production/repositories/2136/datasets/869615371e66021e97829feb7e19e38037ed0c14/GCBS_data.rds'
source('../01-funcoes/f_importar_rds.R')
gcbs <- importar_rds(link)
```

## Conduct a single-factor EFA

Let's begin by using the psych package and conducting a single-factor explanatory factor analysis (EFA). The fa() function from pysch() package conducts an EFA on your data. When you're using this in the real world, be sure to use a dataset that only contains item responses. Take about it, other types of data will cause errors or incorrect results.

We have gcbs dataset, these are examinees' responses to 15 items from the Generic Conspiracist Beliefs Scale, which is designed to measure conspiracist beliefs.

An EFA provides information on each item's relationship to a single factor hypothesized to be represented by each of the items. EFA results give you basic information about how well items relate to that hypothesized construct.

```{r}
EFA_model <- fa(gcbs)

# View the results
EFA_model
```

When you don't specify the number of factors, the fa() function gives us only one factor by default.

```{r}
# View the factor loadings
EFA_model$loadings
```

### Create a path diagram of the items' factor loadings

```{r}
fa.diagram(EFA_model)

# Take a look at the first few lines of the response data and their corresponding sum scores
head(gcbs)
```

> O que é isso?

```{r}
rowSums(head(gcbs))
```

We look at the first few lines of individuals factors scores. This is the result of transformation of data X to a F (Factor).

```{r}
# Then look at the first few lines of individuals' factor scores
head(EFA_model$scores)
```

Distribution of scores:

```{r}
# To get a feel for how the factor scores are distributed, look at their summary statistics and density plot.
summary(EFA_model$scores)
```

Plot a density of scores:

```{r}
plot(density(EFA_model$scores, 
             na.rm = TRUE), 
     main = "Factor Scores")

# dev.off()
# par("mar")
# graphics.off()
```

### Process of delevepment

1.  Develop items for your measure;

2.  Collect pilot data from a representative sample;

3.  Check out what dataset looks like;

    -   3.1 -- Inspect your dataset.

4.  Considere whether you want to use an explanatory analysis (EFA) or a confirmatory (CFA) or still both;

5.  If both, split your sample into random partes;

6.  Compare the two samples to make sure they are similar.

### Splitting your dataset

During the measure development process, it's important to conduct EFA and CFA on separate datasets because using the same dataset can lead to inflated model fit statistics. Instead, you can split your dataset in half, then use one half for the EFA and the other half for the CFA.

```{r}
N <-  nrow(gcbs)
indices <-  seq(1, N)
indices_EFA <- sample(indices, floor(0.5*N))
indices_CFA <- indices[!(indices %in% indices_EFA)]

gcbs_EFA <- gcbs[indices_EFA,]
gcbs_CFA <- gcbs[indices_CFA,]
```

```{r}
# Basic descriptive statistics
describe(gcbs)
```

```{r}
# Graphical representation of error
error.dots((gcbs))
```

```{r}
# Graphical representation of error
error.bars(gcbs)
```

```{r}
# Establish two sets of indices to split the dataset
N <- nrow(gcbs)
indices <- seq(1, N)
indices_EFA <- sample(indices, floor((.5*N)))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
gcbs_EFA <- gcbs[indices_EFA, ]
gcbs_CFA <- gcbs[indices_CFA, ]

# Precisam ter a quantidade igual de dados (diferença máxima de 1)
gcbs_EFA %>% nrow()
gcbs_CFA %>% nrow()

# Use the indices from the previous exercise to create a grouping variable
group_var <- vector("numeric", nrow(gcbs))
group_var[indices_EFA] <- 1
group_var[indices_CFA] <- 2

# Bind that grouping variable onto the gcbs dataset
gcbs_grouped <- cbind(gcbs, group_var)

head(gcbs_grouped)
```

### Compare stats across groups

A word of warning: while the group argument of describeBy() has to be a vector, the group argument of statsBy() has to be the name of a column in your dataframe. Plan accordingly!

```{r}
desc_by_group <- describeBy(gcbs_grouped, group = group_var)
stats_by_group <- statsBy(gcbs_grouped, group = 'group_var')

# formattable::formattable(desc_by_group$'1')
# DT::datatable(desc_by_group$'1') -> conversar com Ariane
```

### Correlation

#### Conduct a single-factor EFA

One of the easiest ways to get a feel for your dataset is to examine the relationships of your variables with each other. While base R has the cor() function, the psych package has the lowerCor() function, which only displays the lower triangle of the correlation matrix for easier viewing and interpretation.

```{r}
# View the lower triangle of the correlation matrix.

knitr::kable(round(lowerCor(gcbs),2))
#formattable::formattable()
```

Vamos usar cores para criar uma matriz semelhante.

```{r}
# Find the correlation
rho_gcbs <- cor(gcbs)

# Incluir NA na parte de baixo do correlação

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

rho_gcbs_upper <- get_upper_tri(rho_gcbs)

# Melt the correlation matrix
melted_rho <- reshape2::melt(rho_gcbs_upper, na.rm = TRUE)

# Alterações no tema
alterar_resto_thema <-
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal")

# Heatmap
ggplot(data = melted_rho, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Matriz\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1))+
  coord_fixed() + 
  geom_text(aes(Var2, Var1, label = round(x = value, digits = 3)), color = "black", size = 3) + 
  alterar_resto_thema


```

Podemos também ver a matriz de correlação completa.

```{r}
# Salvando a Matriz de Correlações 
rho_gcbs <- cor(gcbs)
####################################################################
# Construindo um mapa de calor a partir das correlações
rho_gcbs %>% 
  reshape2::melt() %>% 
  ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = value)) +
  geom_text(aes(x = Var1, y = Var2, label = round(x = value, digits = 2)),
            size = 4) +
  labs(x = NULL,
       y = NULL,
       fill = "Correlações") +
  scale_fill_gradient2(low = "brown4", 
                       mid = "white", 
                       high = "dodgerblue4",
                       midpoint = 0) +
  theme(panel.background = element_rect("white"),
        panel.grid = element_line("grey95"),
        panel.border = element_rect(NA),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 0))
```

#### Test for corr - p-values

```{r}
# Check out the p-values created when calculating correlations. Significant values mean items are meaningfully correlated.
corr.test(gcbs, use = 'pairwise.complete.obs')$p
```

Data set has more than 2000 observations. The statistical significance is affected by sample size.

#### Test for corr - confidente interval

```{r}
# View the confidence intervals created when calculating correlations.
corr.test(gcbs, use = 'pairwise.complete.obs')$ci
```

### Internal reliability

You know how to examine how individual items perform in your measure, but what about how well those items relate to each other - the overall internal reliability of a measure? Coefficient alpha (also called Cronbach's alpha) and split-half reliability are two common ways of assessing reliability. These statistics are a function of the measure length and items' interrelatedness, which you just investigated by looking at the correlation matrix.

In reliability values greater than 0.8 are desired, though some fields of study have higher or lower guidelines.

Resumo: \* Alpha de Cronback - Verifica a consistência da sua medida. Alpha deve estar acima de 0.8 \* O que seria medida? um item ou todos eles juntos?

```{r}
alpha(x = gcbs)
```

```{r}
splitHalf(gcbs)
```

## Conduct a multidimensional-factor EFA

We you use the strategy method of split data in previously section.

```{r}
# Establish two sets of indices to split the dataset
N <-  nrow(bfi)
indices <-  seq(1, N)
indices_EFA <- sample(indices, floor(0.5*N))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
bfi_EFA <- bfi[indices_EFA, ]
bfi_CFA <- bfi[indices_CFA, ]
```

### Calculating eigenvalues

To empirically determine the dimensionality of your data, a common strategy is to examine the eigenvalues. Eigenvalues are numeric representations of the amount of variance explained by each factor or component. Eigenvalues are calculated from a correlation matrix, so you'll need to use cor() to calculate and store the dataset's correlation matrix before calculating eigenvalues. You'll need to specify that you want to use pairwise complete observations. The default is to use everything, but if your dataset has any missing values, this will leave you with a matrix full of NAs.

You'll do these calculations on the bfi_EFA dataset you just created - remember, you're saving the data in bfi_CFA for your confirmatory analysis!

```{r}
# Calculate the correlation matrix first
bfi_EFA_cor <- cor(bfi_EFA, use = 'pairwise.complete.obs')

# Then use that correlation matrix to calculate eigenvalues
eigenvals <- eigen(bfi_EFA_cor)

# Look at the eigenvalues returned
eigenvals$values
```

### Creating a scree plot - representation of eigenvalues

Eigenvalues can be generated from a principal component analysis or a factor analysis, and the scree() function calculates and plots both by default. Since eigen() finds eigenvalues via principal components analysis, we will use factors = FALSE so our scree plot will only display the values corresponding to those results.

```{r}
par(mfrow = c(1,2))
# Calculate the correlation matrix first 
bfi_EFA_cor <- cor(bfi_EFA, use = 'pairwise.complete.obs')

# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor, factors = FALSE)

# See the difference to factor = TRUE
# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor, factors = TRUE)
```

A commonly used criterion for selecting the optimal number of factors is to only consider factors with eigenvalues greater than 1. scree() includes a solid horizontal line at 1 on the y-axis to help you quickly interpret your results. Run the code below to recreate the scree plot from the bfi_EFA data you created in the previous exercise. Based on the results, how many factors are recommended?

> R: 6 factors

### Understanding multidimensional data--

-   Construct: an attribute of interesting
-   Each factor correspond to one construct

Now that you've examined the eigenvalues and scree plot to find the data-driven recommended number of factors, you can get down to actually running the multidimensional EFA. In Chapter 1, you ran a unidimensional EFA by using the fa() function. To run a multidimensional EFA, you'll want to use the nfactors argument to specify the number of factors desired.

```{r}
# Run the EFA with six factors (as indicated by your scree plot)
EFA_model <- fa(bfi_EFA, nfactors = 6)

# View results from the model object
EFA_model
```

### Interpreting the results

As before, you'll be interested in items' factor loadings and individuals' factor scores. These will be interpreted in the same way, but since your EFA is multidimensional, you'll get results for each factor.

Remember, an item's loadings represent the amount of information it provides for each factor. Items' meaningful loadings will be displayed in the output. You'll notice that many items load onto more than one factor, which means they provide information about multiple factors. This may not be desirable for measure development, so some researchers consider only the strongest loading for each item.

```{r}
# View items' factor loadings
EFA_model$loadings

# View the first few lines of examinees' factor scores
head(EFA_model$scores)
```

### Model Fit

#### Selecting the best model

Now use your knowledge of finding and interpreting absolute and relative model fit statistics to select the best model for your data. When I introduced this dataset I said that the items were theorized to load onto five factors, but you may have noticed that your scree plot indicated six factors. You might be wondering which you should trust. Not to worry - you can use fit statistics to make am empirical decision about how many factors to use.

First, you'll use the bfi_EFA dataset to run EFAs with each of the hypothesized number of factors. Then, you can look at the BIC, which is a relative fit statistic, to compare models. Remember, the lowest BIC is preferred!

```{r}
# Run each theorized EFA on your dataset
bfi_theory <- fa(bfi_EFA, nfactors = 5)
bfi_eigen <- fa(bfi_EFA, nfactors = 6)


# Compare the BIC values
bfi_theory$BIC
bfi_eigen$BIC
```

## Referências

-   [Course Datacamp: Factor Analysis (course 1 and 2)](https://app.datacamp.com/learn/courses/factor-analysis-in-r) - Basis to this article

-   <https://willstenico.medium.com/puffindex-criando-um-ranking-de-a%C3%A7%C3%B5es-brasileiras-utilizando-pca-analise-de-componentes-2715f2ffc46a>

-   <https://aaronschlegel.me/factor-analysis-principal-component-method-r.html> (very good)

-   <http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization>

-   <https://www.r-graph-gallery.com/199-correlation-matrix-with-ggally.html>

-   <https://willstenico.medium.com/puffindex-criando-um-ranking-de-a%C3%A7%C3%B5es-brasileiras-utilizando-pca-analise-de-componentes-2715f2ffc46a>

-   <https://rpubs.com/aaronsc32/factor-analysis-principal-factor-method>

-   <https://www.ernestoamaral.com/docs/dcp046-111/Aula28.pdf>

-   <https://www.ime.usp.br/~pavan/pdf/PCA-R-2013>

-   <https://rpubs.com/aaronsc32/factor-analysis-principal-factor-method>
